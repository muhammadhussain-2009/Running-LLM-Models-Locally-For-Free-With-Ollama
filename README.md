<div id="top">

<!-- HEADER STYLE: CLASSIC -->
<div align="center">

# RUNNING-LLM-MODELS-LOCALLY-FOR-FREE-WITH-OLLAMA

<em>Unleash Powerful AI, Locally and Cost-Effectively</em>

<!-- BADGES -->
<img src="https://img.shields.io/github/license/muhammadhussain-2009/Running-LLM-Models-Locally-For-Free-With-Ollama?style=flat&logo=opensourceinitiative&logoColor=white&color=0080ff" alt="license">
<img src="https://img.shields.io/github/last-commit/muhammadhussain-2009/Running-LLM-Models-Locally-For-Free-With-Ollama?style=flat&logo=git&logoColor=white&color=0080ff" alt="last-commit">
<img src="https://img.shields.io/github/languages/top/muhammadhussain-2009/Running-LLM-Models-Locally-For-Free-With-Ollama?style=flat&color=0080ff" alt="repo-top-language">
<img src="https://img.shields.io/github/languages/count/muhammadhussain-2009/Running-LLM-Models-Locally-For-Free-With-Ollama?style=flat&color=0080ff" alt="repo-language-count">

<em>Built with the tools and technologies:Python, VSCode, Ollama, Gemma3</em>

<img src="https://img.shields.io/badge/Markdown-000000.svg?style=flat&logo=Markdown&logoColor=white" alt="Markdown">
<img src="https://img.shields.io/badge/Python-3776AB.svg?style=flat&logo=Python&logoColor=white" alt="Python">

</div>
<br>

---

## 📄 Table of Contents

- [Overview](#-overview)
- [Getting Started](#-getting-started)
    - [Prerequisites](#-prerequisites)
    - [Installation](#-installation)
    - [Usage](#-usage)
    - [Testing](#-testing)
- [Features](#-features)
- [Project Structure](#-project-structure)
    - [Project Index](#-project-index)
- [License](#-license)

---

## ✨ Overview

Running-LLM-Models-Locally-For-Free-With-Ollama enables developers to deploy and interact with large language models locally, providing a cost-effective and flexible environment for AI experimentation. The core features include:

- 🧩 **🚀 Fast Local Deployment:** Easily set up and run LLMs on your machine using Ollama, avoiding cloud costs.
- 🎯 **🔄 Real-Time Streaming:** Send chat prompts and receive live, streamed responses for dynamic conversational experiences.
- 🛠️ **🔗 Seamless API Integration:** Interact with the Ollama API through simple scripts, enabling automated and scalable workflows.
- ⚙️ **🧰 Modular Components:** Core scripts like `test.py` and `package.py` facilitate flexible integration into larger AI systems.
- 💡 **📚 Developer-Friendly:** Clear setup guides and example code streamline local experimentation and development.
- 🔬 **🥼 Inspiration Taken From:** https://www.youtube.com/watch?v=UtSSMs6ObqY&t=639s

---

## 📌 Features

|      | Component       | Details                                                                                     |
| :--- | :-------------- | :------------------------------------------------------------------------------------------ |
| ⚙️  | **Architecture**  | <ul><li>Client-Server Model: Local CLI interface interacts with Ollama API</li><li>Modular separation between model management and execution</li></ul> |
| 🔩 | **Code Quality**  | <ul><li>Clear Python scripts with modular functions</li><li>Consistent naming conventions</li></ul> |
| 📄 | **Documentation** | <ul><li>README provides setup and usage instructions</li><li>Inline comments in code for clarity</li></ul> |
| 🔌 | **Integrations**  | <ul><li>Ollama CLI for local LLM deployment</li><li>Python for scripting and automation</li></ul> |
| 🧩 | **Modularity**    | <ul><li>Separate modules for model selection, inference, and utilities</li><li>Easy to extend with new models or features</li></ul> |
| 🧪 | **Testing**       | <ul><li>Limited unit tests present, primarily for utility functions</li><li>Potential for expanded test coverage</li></ul> |
| ⚡️  | **Performance**   | <ul><li>Runs models locally, reducing latency compared to cloud</li><li>Efficient use of Ollama's optimized inference</li></ul> |
| 🛡️ | **Security**      | <ul><li>Minimal security features; relies on local environment</li><li>No explicit authentication or encryption mechanisms</li></ul> |
| 📦 | **Dependencies**  | <ul><li>Python 3.x environment</li><li>Ollama CLI installed and configured</li><li>Markdown for documentation</li></ul> |

---

## 📁 Project Structure

```sh
└── Running-LLM-Models-Locally-For-Free-With-Ollama/
    ├── LICENSE
    ├── README.md
    ├── package.py
    └── test.py
```

---

### 📑 Project Index

<details open>
	<summary><b><code>RUNNING-LLM-MODELS-LOCALLY-FOR-FREE-WITH-OLLAMA/</code></b></summary>
	<!-- __root__ Submodule -->
	<details>
		<summary><b>__root__</b></summary>
		<blockquote>
			<div class='directory-path' style='padding: 8px 0; color: #666;'>
				<code><b>⦿ __root__</b></code>
			<table style='width: 100%; border-collapse: collapse;'>
			<thead>
				<tr style='background-color: #f8f9fa;'>
					<th style='width: 30%; text-align: left; padding: 8px;'>File Name</th>
					<th style='text-align: left; padding: 8px;'>Summary</th>
				</tr>
			</thead>
				<tr style='border-bottom: 1px solid #eee;'>
					<td style='padding: 8px;'><b><a href='https://github.com/muhammadhussain-2009/Running-LLM-Models-Locally-For-Free-With-Ollama/blob/master/test.py'>test.py</a></b></td>
					<td style='padding: 8px;'>- Facilitates interaction with the local Ollama API by sending a chat prompt and streaming the assistants response<br>- It enables real-time retrieval of AI-generated answers, integrating seamlessly into the broader architecture for conversational AI workflows<br>- This script exemplifies how to leverage the API for dynamic, streamed communication within the project ecosystem.</td>
				</tr>
				<tr style='border-bottom: 1px solid #eee;'>
					<td style='padding: 8px;'><b><a href='https://github.com/muhammadhussain-2009/Running-LLM-Models-Locally-For-Free-With-Ollama/blob/master/README.md'>README.md</a></b></td>
					<td style='padding: 8px;'>- Provides an overview of how to set up and run large language models locally using Ollama, facilitating accessible and cost-effective experimentation with LLMs<br>- The guide integrates with the broader architecture by enabling developers to deploy and test models efficiently within the project’s ecosystem, supporting scalable AI workflows and enhancing local development capabilities.</td>
				</tr>
				<tr style='border-bottom: 1px solid #eee;'>
					<td style='padding: 8px;'><b><a href='https://github.com/muhammadhussain-2009/Running-LLM-Models-Locally-For-Free-With-Ollama/blob/master/package.py'>package.py</a></b></td>
					<td style='padding: 8px;'>- Facilitates interaction with the Ollama AI platform by initializing a client, selecting a specific language model, and submitting a prompt to generate a response<br>- Serves as a core component for integrating AI-driven query processing within the broader architecture, enabling automated natural language understanding and response generation for various applications.</td>
				</tr>
				<tr style='border-bottom: 1px solid #eee;'>
					<td style='padding: 8px;'><b><a href='https://github.com/muhammadhussain-2009/Running-LLM-Models-Locally-For-Free-With-Ollama/blob/master/LICENSE'>LICENSE</a></b></td>
					<td style='padding: 8px;'>- Provides the core licensing information for the project, establishing legal permissions and restrictions<br>- It ensures users understand their rights to use, modify, and distribute the software while clarifying liability limitations<br>- This file underpins the projects open-source distribution, supporting its integration and collaboration within the broader codebase architecture.</td>
				</tr>
			</table>
		</blockquote>
	</details>
</details>

---

## 🚀 Getting Started

### 📋 Prerequisites

This project requires the following dependencies:

- **Programming Language:** Python in VS Code 
- **Package Manager:** Conda

### ⚙️ Installation

Build Running-LLM-Models-Locally-For-Free-With-Ollama from the source and install dependencies:

1. **Clone the repository:**

    ```sh
    ❯ git clone https://github.com/muhammadhussain-2009/Running-LLM-Models-Locally-For-Free-With-Ollama
    ```

2. **Navigate to the project directory:**

    ```sh
    ❯ cd Running-LLM-Models-Locally-For-Free-With-Ollama
    ```

3. **Install the dependencies:**

**Using [conda](https://docs.conda.io/):**

```sh
❯ conda env create -f conda.yml
```

### 💻 Usage

Run the project with:

**Using [conda](https://docs.conda.io/):**

```sh
conda activate {venv}
python {entrypoint}
```

### 🧪 Testing

Running-llm-models-locally-for-free-with-ollama uses the {__test_framework__} test framework. Run the test suite with:

**Using [conda](https://docs.conda.io/):**

```sh
conda activate {venv}
pytest
```

---

## 📜 License

Running-llm-models-locally-for-free-with-ollama is protected under the [LICENSE](https://choosealicense.com/licenses) License. For more details, refer to the [LICENSE](https://choosealicense.com/licenses/) file.

---

<div align="left"><a href="#top">⬆ Return</a></div>

---
